# Base Environment Configuration Template
# Copy this file to .env and customize for your local development

# Environment identifier
AI_ENVIRONMENT=development

# =============================================================================
# LLM Provider API Keys
# =============================================================================
# Get your API keys from:
# - OpenAI: https://platform.openai.com/api-keys
# - Gemini: https://aistudio.google.com/app/apikey  
# - Anthropic: https://console.anthropic.com/account/keys
# - Groq: https://console.groq.com/keys

# OpenAI API Key (required for orchestrator and agent)
OPENAI_API_KEY=sk-your-openai-key-here

# Gemini API Key (optional, for file analysis)
GEMINI_API_KEY=your-gemini-key-here

# Anthropic API Key (optional, for chat interfaces)
ANTHROPIC_API_KEY=your-anthropic-key-here

# Groq API Key (optional, for fast development iterations)
GROQ_API_KEY=your-groq-key-here

# =============================================================================
# Local Development Settings
# =============================================================================

# Debug mode (enables verbose logging)
DEBUG=true

# Log level for development
LOG_LEVEL=debug

# Enable development features
ENABLE_HOT_RELOAD=true

# =============================================================================
# Local Model Configurations (Optional)
# =============================================================================

# Ollama server for local models
OLLAMA_HOST=http://localhost:11434

# Gemini CLI path (for local testing)
GEMINI_CLI_PATH=gemini

# =============================================================================
# Development Overrides
# =============================================================================

# Use faster models for development (cost optimization)
# Uncomment to override model assignments:
# FORCE_FAST_MODELS=true

# Enable mock responses for testing without API calls
# MOCK_LLM_RESPONSES=false
